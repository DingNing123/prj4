{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c57c68-9a03-4d77-a686-0675ecc9f854",
   "metadata": {},
   "source": [
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7504e4a6-04d1-4691-82d6-7feacca1c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str,\n",
    "                    choices=[\"mosi\", \"mosei\"], default=\"mosi\")\n",
    "parser.add_argument(\"--max_seq_length\", type=int, default=50)\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=48)\n",
    "parser.add_argument(\"--dev_batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--test_batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--beta_shift\", type=float, default=1.0)\n",
    "parser.add_argument(\"--dropout_prob\", type=float, default=0.5)\n",
    "parser.add_argument(\n",
    "    \"--model\",\n",
    "    type=str,\n",
    "    choices=[\"bert-base-uncased\"],\n",
    "    default=\"bert-base-uncased\",\n",
    ")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--gradient_accumulation_step\", type=int, default=1)\n",
    "parser.add_argument(\"--warmup_proportion\", type=float, default=0.1)\n",
    "parser.add_argument(\"--seed\", type=int, default=5576)\n",
    "parser.add_argument(\"--mib\", type=str, default='cmib')\n",
    "# python main_mib.py --mib emib --dataset mosi --train_batch_size 32 --n_epochs 50\n",
    "args = parser.parse_args(args=['--mib','emib','--train_batch_size','2','--n_epochs','2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d312f-c82f-46f9-b0e0-07b6b6c9c9a2",
   "metadata": {},
   "source": [
    "# add path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea566f83-cf14-4c6a-924d-15d7c6b74c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Because notebooks are placed in subdirectories, add the path of the root directory\n",
    "sys.path.append(\"..\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00459d-f36f-4a61-9edf-95959223362f",
   "metadata": {},
   "source": [
    "# import MIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e11d73e-2810-4324-9085-c14a069e51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84bc18c-a6fd-45a2-ba51-7dd770e767c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting emib!\n"
     ]
    }
   ],
   "source": [
    "if args.mib == 'cmib':\n",
    "    from cmib import MIB\n",
    "    print('selecting cmib!')\n",
    "elif args.mib == 'emib':\n",
    "    from emib import MIB\n",
    "    print('selecting emib!')\n",
    "elif args.mib == 'lmib':\n",
    "    from lmib import MIB\n",
    "    print('selecting lmib!')\n",
    "else:\n",
    "    print('error! you should choose from {cmib,emib,lmib}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a014b48-e3d7-453e-ae36-7105da0ace7d",
   "metadata": {},
   "source": [
    "transformer version problem:\n",
    "\n",
    "my version transformers 4.14.1\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "transformers=3.0.2=pypi_0\n",
    "\n",
    "from transformers.modeling_bert import BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea199a00-33dd-4319-90f7-2cce77a6f7c5",
   "metadata": {},
   "source": [
    "# def set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec844a16-834d-4906-a9a3-dcc560aeb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function to seed experiment for reproducibility.\n",
    "    If -1 is provided as seed, experiment uses random seed from 0~9999\n",
    "\n",
    "    Args:\n",
    "        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db1b15-29c3-4ed2-bd3d-2c5a57cc1f58",
   "metadata": {},
   "source": [
    "# global_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c1187c-2f72-4579-8346-3aa089891608",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = '/Users/mac/Desktop/tools/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479c89d-d88e-4891-8761-b08fdbb92439",
   "metadata": {},
   "source": [
    "# def get_tokenizer(model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417a1e9b-6174-406b-883f-2103502a0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, XLNetTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "def get_tokenizer(model):\n",
    "    if model == \"bert-base-uncased\":\n",
    "        # bert_path = '/Users/mac/Desktop/tools/bert-base-uncased'\n",
    "        return BertTokenizer.from_pretrained(bert_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Expected 'bert-base-uncased' or 'xlnet-base-cased, but received {}\".format(\n",
    "                model\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091dcad-81fa-4266-9132-35a811571310",
   "metadata": {},
   "source": [
    "# def prepare_bert_input(tokens, visual, acoustic, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14429626-345e-43c7-92f3-164e260b76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_configs import TEXT_DIM, ACOUSTIC_DIM, VISUAL_DIM, DEVICE\n",
    "\n",
    "def prepare_bert_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    tokens = [CLS] + tokens + [SEP]\n",
    "\n",
    "    # Pad zero vectors for acoustic / visual vectors to account for [CLS] / [SEP] tokens\n",
    "    acoustic_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic_zero, acoustic, acoustic_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual_zero, visual, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    pad_length = args.max_seq_length - len(input_ids)\n",
    "\n",
    "    acoustic_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, acoustic_padding))\n",
    "\n",
    "    visual_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_padding))\n",
    "\n",
    "    padding = [0] * pad_length\n",
    "\n",
    "    # Pad inputs\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b736266-8595-4b56-8fac-99b8d3cf9080",
   "metadata": {},
   "source": [
    "# class InputFeatures(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e43a164-281b-43ea-a2fa-e0dd32e223c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, visual, acoustic, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.visual = visual\n",
    "        self.acoustic = acoustic\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803d1c5-b437-4255-96b6-65bfcbaee07b",
   "metadata": {},
   "source": [
    "# def convert_to_features(examples, max_seq_length, tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932e79e6-e5c2-4b84-96ad-d76f093585a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "\n",
    "        (words, visual, acoustic), label_id, segment = example\n",
    "       # print(words)\n",
    "        tokens, inversions = [], []\n",
    "        for idx, word in enumerate(words):\n",
    "            tokenized = tokenizer.tokenize(word)\n",
    "           # print(tokenized)\n",
    "            tokens.extend(tokenized)\n",
    "            inversions.extend([idx] * len(tokenized))\n",
    "\n",
    "        # Check inversion\n",
    "        assert len(tokens) == len(inversions)\n",
    "\n",
    "        aligned_visual = []\n",
    "        aligned_audio = []\n",
    "\n",
    "        for inv_idx in inversions:\n",
    "            aligned_visual.append(visual[inv_idx, :])\n",
    "            aligned_audio.append(acoustic[inv_idx, :])\n",
    "\n",
    "        visual = np.array(aligned_visual)\n",
    "        acoustic = np.array(aligned_audio)\n",
    "\n",
    "        # Truncate input if necessary\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[: max_seq_length - 2]\n",
    "            acoustic = acoustic[: max_seq_length - 2]\n",
    "            visual = visual[: max_seq_length - 2]\n",
    "\n",
    "        if args.model == \"bert-base-uncased\":\n",
    "            prepare_input = prepare_bert_input\n",
    "        elif args.model == \"xlnet-base-cased\":\n",
    "            prepare_input = prepare_xlnet_input\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids = prepare_input(\n",
    "            tokens, visual, acoustic, tokenizer\n",
    "        )\n",
    "\n",
    "        # Check input length\n",
    "        assert len(input_ids) == args.max_seq_length\n",
    "        assert len(input_mask) == args.max_seq_length\n",
    "        assert len(segment_ids) == args.max_seq_length\n",
    "        assert acoustic.shape[0] == args.max_seq_length\n",
    "        assert visual.shape[0] == args.max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                visual=visual,\n",
    "                acoustic=acoustic,\n",
    "                label_id=label_id,\n",
    "            )\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb6a8c-fff6-406d-99b3-bc7e443a26ad",
   "metadata": {},
   "source": [
    "# get_appropriate_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05faa2d-3136-4c57-b57e-12413a06f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "debug = True\n",
    "def get_appropriate_dataset(data):\n",
    "\n",
    "    tokenizer = get_tokenizer(args.model)\n",
    "\n",
    "    features = convert_to_features(data, args.max_seq_length, tokenizer)\n",
    "    all_input_ids = torch.tensor(\n",
    "        [f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(\n",
    "        [f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(\n",
    "        [f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
    "    all_acoustic = torch.tensor(\n",
    "        [f.acoustic for f in features], dtype=torch.float)\n",
    "    all_label_ids = torch.tensor(\n",
    "        [f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    if debug:\n",
    "        dataset = TensorDataset(\n",
    "        all_input_ids[:4],\n",
    "        all_visual[:4],\n",
    "        all_acoustic[:4],\n",
    "        all_input_mask[:4],\n",
    "        all_segment_ids[:4],\n",
    "        all_label_ids[:4],\n",
    "    )\n",
    "    else:\n",
    "        dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_visual,\n",
    "        all_acoustic,\n",
    "        all_input_mask,\n",
    "        all_segment_ids,\n",
    "        all_label_ids,\n",
    "    )\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6333a9-75c7-4e2b-927c-1de84abfec02",
   "metadata": {},
   "source": [
    "# def set_up_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69308728-4f00-4590-b5c0-b891783bbc22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "def set_up_data_loader():\n",
    "    # /Users/mac/Desktop/big_file/mosi.pkl\n",
    "    with open(f\"/Users/mac/Desktop/big_file/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    train_data = data[\"train\"]\n",
    "    dev_data = data[\"dev\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    train_dataset = get_appropriate_dataset(train_data)\n",
    "    dev_dataset = get_appropriate_dataset(dev_data)\n",
    "    test_dataset = get_appropriate_dataset(test_data)\n",
    "\n",
    "    num_train_optimization_steps = (\n",
    "        int(\n",
    "            len(train_dataset) / args.train_batch_size /\n",
    "            args.gradient_accumulation_step\n",
    "        )\n",
    "        * args.n_epochs\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset, batch_size=args.dev_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        test_dataloader,\n",
    "        num_train_optimization_steps,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f499c1c-c747-4d1d-af8c-1e015c6c2aa1",
   "metadata": {},
   "source": [
    "# class MultimodalConfig(object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53fa156f-28a4-47af-b5d1-e260f1feb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7c047-dbd1-44ce-9364-ece4b1122ebe",
   "metadata": {},
   "source": [
    "# def prep_for_training(num_train_optimization_steps: int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc69479-659c-424c-90b0-00989c4f2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_training(num_train_optimization_steps: int):\n",
    "    multimodal_config = MultimodalConfig(\n",
    "        beta_shift=args.beta_shift, dropout_prob=args.dropout_prob\n",
    "    )   \n",
    "\n",
    "    if args.model == \"bert-base-uncased\":\n",
    "        model = MIB.from_pretrained(\n",
    "            bert_path, multimodal_config=multimodal_config, num_labels=1,\n",
    "        )   \n",
    "   \n",
    "\n",
    "    total_para = 0 \n",
    "    for param in model.parameters():\n",
    "        total_para += np.prod(param.size())\n",
    "    print('total parameter for the model: ', total_para)\n",
    "\n",
    "\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1fb13-8200-4014-9848-b3ef6cfe1167",
   "metadata": {},
   "source": [
    "# def train_epoch(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d9f117-d5c4-42cc-a21b-b0222d876466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import CrossEntropyLoss, L1Loss, MSELoss\n",
    "\n",
    "def train_epoch(model: nn.Module, train_dataloader: DataLoader):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            label_ids,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None,\n",
    "        )\n",
    "\n",
    "        logits = outputs #+ outputa + outputv\n",
    "\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if args.gradient_accumulation_step > 1:\n",
    "            loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "     \n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "       \n",
    "\n",
    "    return tr_loss / nb_tr_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a2777-26e2-4cb5-84ca-715174964731",
   "metadata": {},
   "source": [
    "# def eval_epoch(model, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94220245-fa6d-456b-93c1-86817362f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model: nn.Module, dev_dataloader: DataLoader):\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    nb_dev_examples, nb_dev_steps = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model.test(\n",
    "                input_ids,\n",
    "                 visual,\n",
    "                 acoustic,\n",
    "                # label_ids,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "               # labels=None,\n",
    "            )\n",
    "\n",
    "            logits = outputs\n",
    "\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "            if args.gradient_accumulation_step > 1:\n",
    "                loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "            dev_loss += loss.item()\n",
    "            nb_dev_steps += 1\n",
    "\n",
    "    return dev_loss / nb_dev_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690f636-acc2-453c-904e-95cb09ba8063",
   "metadata": {},
   "source": [
    "# def test_epoch(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10365a0c-4d31-4bb4-b143-2aa2266f5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model: nn.Module, test_dataloader: DataLoader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model.test(\n",
    "                input_ids,\n",
    "                 visual,\n",
    "                 acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None,\n",
    "            )\n",
    "\n",
    "            logits = outputs\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "    return preds, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa024293-bca9-4722-9cd6-f32f89bf5c98",
   "metadata": {},
   "source": [
    "# def multiclass_acc(test_preds_a7, test_truth_a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f721d50-ae89-4bd0-8ab7-303d31f8dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_acc(preds, truths):\n",
    "    \"\"\" \n",
    "    Compute the multiclass accuracy w.r.t. groundtruth\n",
    "\n",
    "    :param preds: Float array representing the predictions, dimension (N,)\n",
    "    :param truths: Float/int array representing the groundtruth classes, dimension (N,)\n",
    "    :return: Classification accuracy\n",
    "    \"\"\"\n",
    "    return np.sum(np.round(preds) == np.round(truths)) / float(len(truths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714fe6c-ee85-4511-a037-2e9830763dd7",
   "metadata": {},
   "source": [
    "# def test_score_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c60d54df-26d1-49f3-b5a0-a4328df9b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def test_score_model(model: nn.Module, test_dataloader: DataLoader, use_zero=False):\n",
    "\n",
    "    preds, y_test = test_epoch(model, test_dataloader)\n",
    "    non_zeros = np.array(\n",
    "        [i for i, e in enumerate(y_test) if e != 0 or use_zero])\n",
    "\n",
    "    test_preds_a7 = np.clip(preds, a_min=-3., a_max=3.)\n",
    "    test_truth_a7 = np.clip(y_test, a_min=-3., a_max=3.)\n",
    "    mult_a7 = multiclass_acc(test_preds_a7, test_truth_a7)\n",
    "\n",
    "\n",
    "    preds = preds[non_zeros]\n",
    "    y_test = y_test[non_zeros]\n",
    "\n",
    "\n",
    "\n",
    "    mae = np.mean(np.absolute(preds - y_test))\n",
    "    corr = np.corrcoef(preds, y_test)[0][1]\n",
    "\n",
    "    preds = preds >= 0\n",
    "    y_test = y_test >= 0\n",
    "\n",
    "    f_score = f1_score(y_test, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "\n",
    "    return acc, mae, corr, f_score, mult_a7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90829f5d-f9e9-4eb7-8ee0-bed37565a784",
   "metadata": {},
   "source": [
    "# def train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8363776c-a1b0-44ed-8544-328ea335d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    test_data_loader\n",
    "):\n",
    "    valid_losses = []\n",
    "    test_accuracies = []\n",
    "    best_loss = 10\n",
    "    for epoch_i in range(int(args.n_epochs)):\n",
    "        train_loss = train_epoch(model, train_dataloader)\n",
    "        valid_loss = eval_epoch(model, validation_dataloader)\n",
    "        test_acc, test_mae, test_corr, test_f_score, test_acc7 = test_score_model(\n",
    "            model, test_data_loader\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"epoch:{}, train_loss:{:.4f}, valid_loss:{:.4f}, test_acc:{:.4f}\".format(\n",
    "                epoch_i, train_loss, valid_loss, test_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        print(\n",
    "            \"current mae:{:.4f}, current acc:{:.4f}, acc7:{:.4f}, f1:{:.4f}, corr:{:.4f}\".format(\n",
    "                test_mae, test_acc, test_acc7, test_f_score, test_corr\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        valid_losses.append(valid_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_acc = test_acc\n",
    "            best_mae = test_mae\n",
    "            best_corr = test_corr\n",
    "            best_f_score = test_f_score\n",
    "            best_acc_7 = test_acc7\n",
    "        print(\n",
    "            \"best mae:{:.4f}, acc:{:.4f}, acc7:{:.4f}, f1:{:.4f}, corr:{:.4f}\".format(\n",
    "                best_mae, best_acc, best_acc_7, best_f_score, best_corr\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        wandb.log(\n",
    "            (\n",
    "                {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"valid_loss\": valid_loss,\n",
    "                    \"test_acc\": test_acc,\n",
    "                    \"test_mae\": test_mae,\n",
    "                    \"test_corr\": test_corr,\n",
    "                    \"test_f_score\": test_f_score,\n",
    "                    \"test_acc7\": test_acc7,\n",
    "                    \"best_valid_loss\": min(valid_losses),\n",
    "                    \"best_test_acc\": max(test_accuracies),\n",
    "                }\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52381c-d5db-4ab9-8200-761c60227a35",
   "metadata": {},
   "source": [
    "# def main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e633b12-f98d-4262-a259-f4698da5646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def main():\n",
    "    print(args)\n",
    "    wandb.init(project=\"prj4\")\n",
    "    wandb.config.update(args)\n",
    "    set_random_seed(args.seed)\n",
    "    (\n",
    "        train_data_loader,\n",
    "        dev_data_loader,\n",
    "        test_data_loader,\n",
    "        num_train_optimization_steps,\n",
    "    ) = set_up_data_loader()\n",
    "    # print(len(next(iter(train_data_loader))))\n",
    "    model = prep_for_training(\n",
    "        num_train_optimization_steps)\n",
    "    \n",
    "    train(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        dev_data_loader,\n",
    "        test_data_loader\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ddf10-7087-4e79-9b18-0458bd11529b",
   "metadata": {},
   "source": [
    "# run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "109bf98d-1ea2-4d1e-a7d3-b5d4442495c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(beta_shift=1.0, dataset='mosi', dev_batch_size=128, dropout_prob=0.5, gradient_accumulation_step=1, learning_rate=1e-05, max_seq_length=50, mib='emib', model='bert-base-uncased', n_epochs=2, seed=5576, test_batch_size=128, train_batch_size=2, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdingning\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mac/Desktop/prj4/note_books/wandb/run-20221219_141426-2mhteaqx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dingning/prj4/runs/2mhteaqx\" target=\"_blank\">laced-star-7</a></strong> to <a href=\"https://wandb.ai/dingning/prj4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/mac/Desktop/tools/bert-base-uncased were not used when initializing MIB: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing MIB from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MIB from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MIB were not initialized from the model checkpoint at /Users/mac/Desktop/tools/bert-base-uncased and are newly initialized: ['fusion.encoder_a.0.weight', 'fusion.encoder_v.0.weight', 'fusion.fc_mu_a.bias', 'transa.layers.2.layer_norms.0.weight', 'transa.layers.0.layer_norms.0.bias', 'transv.layers.1.self_attn.out_proj.weight', 'fusion.fc_mu_a.weight', 'transa.layers.0.layer_norms.1.bias', 'fusion.fc_std.weight', 'transa.layers.1.fc1.bias', 'transv.embed_positions._float_tensor', 'transa.layers.2.fc2.weight', 'fusion.decoder_v.bias', 'fusion.decoder_v.weight', 'transa.layers.2.layer_norms.1.weight', 'transv.layers.2.self_attn.in_proj_weight', 'fusion.fc_std_l.weight', 'transv.layers.1.self_attn.out_proj.bias', 'proj_v.weight', 'transv.layers.2.self_attn.out_proj.bias', 'transa.layers.0.fc1.bias', 'fusion.decoder_a.bias', 'transa.layers.1.fc2.weight', 'transa.layers.2.fc1.bias', 'transa.layers.1.self_attn.out_proj.weight', 'transa.layers.1.layer_norms.0.bias', 'fusion.fc_std_a.weight', 'transa.layers.0.fc2.weight', 'transv.layers.1.fc1.bias', 'fusion.fc_std_a.bias', 'transv.layer_norm.weight', 'transa.layers.2.self_attn.in_proj_bias', 'transv.layers.2.self_attn.in_proj_bias', 'transa.layers.2.fc1.weight', 'fusion.fusion1.linear_1.bias', 'fusion.fc_std_v.weight', 'proj_a.weight', 'transa.version', 'transv.layers.0.fc2.weight', 'transv.layers.0.layer_norms.1.weight', 'proj_l.weight', 'fusion.decoder.weight', 'transa.layer_norm.bias', 'transv.layers.2.layer_norms.0.weight', 'fusion.decoder.bias', 'fusion.encoder_l.0.bias', 'fusion.fc_std_v.bias', 'transv.layers.1.self_attn.in_proj_bias', 'transa.layers.1.layer_norms.1.bias', 'transa.layers.0.self_attn.out_proj.weight', 'transa.layers.2.layer_norms.1.bias', 'transv.layers.0.fc1.bias', 'fusion.fc_std_l.bias', 'transa.embed_positions._float_tensor', 'transv.layers.2.fc1.weight', 'transv.layers.0.fc2.bias', 'transv.layers.1.layer_norms.0.weight', 'transv.layers.1.fc2.bias', 'fusion.fc_mu_v.bias', 'transv.layers.0.layer_norms.0.bias', 'transa.layers.1.layer_norms.0.weight', 'transv.layers.0.self_attn.in_proj_weight', 'fusion.fc_mu_v.weight', 'fusion.decoder_a.weight', 'transv.layers.0.self_attn.in_proj_bias', 'transv.layers.0.layer_norms.0.weight', 'fusion.decoder_l.weight', 'transv.layers.0.self_attn.out_proj.weight', 'transv.layers.2.fc2.weight', 'transv.layers.1.self_attn.in_proj_weight', 'transv.layers.0.layer_norms.1.bias', 'transv.layers.1.fc2.weight', 'transa.layers.0.layer_norms.0.weight', 'transv.layer_norm.bias', 'transa.layers.0.self_attn.in_proj_weight', 'fusion.fc_mu_l.bias', 'transv.layers.2.fc2.bias', 'transv.layers.2.fc1.bias', 'transa.layers.1.self_attn.out_proj.bias', 'transa.layers.0.self_attn.out_proj.bias', 'transa.layers.2.layer_norms.0.bias', 'transv.version', 'transv.layers.0.fc1.weight', 'transv.layers.2.layer_norms.0.bias', 'transa.layers.1.self_attn.in_proj_bias', 'fusion.encoder.0.weight', 'fusion.fc_mu_l.weight', 'transa.layers.0.fc2.bias', 'transv.layers.1.fc1.weight', 'transa.layers.2.self_attn.out_proj.weight', 'transv.layers.0.self_attn.out_proj.bias', 'transa.layers.2.self_attn.in_proj_weight', 'transa.layers.2.self_attn.out_proj.bias', 'transa.layers.0.layer_norms.1.weight', 'fusion.fc_std.bias', 'fusion.encoder.0.bias', 'transv.layers.1.layer_norms.0.bias', 'fusion.fusion1.linear_1.weight', 'transv.layers.2.layer_norms.1.weight', 'transa.layers.1.layer_norms.1.weight', 'transa.layers.0.fc1.weight', 'transa.layers.1.fc1.weight', 'transv.layers.1.layer_norms.1.bias', 'fusion.encoder_l.0.weight', 'transa.layers.1.fc2.bias', 'fusion.fc_mu.bias', 'transa.layer_norm.weight', 'fusion.encoder_v.0.bias', 'transv.layers.2.self_attn.out_proj.weight', 'bert.proj_l.weight', 'transa.layers.2.fc2.bias', 'transv.layers.1.layer_norms.1.weight', 'fusion.encoder_a.0.bias', 'transa.layers.0.self_attn.in_proj_bias', 'fusion.decoder_l.bias', 'fusion.fc_mu.weight', 'transv.layers.2.layer_norms.1.bias', 'transa.layers.1.self_attn.in_proj_weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameter for the model:  109950948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                           | 0/2 [00:00<?, ?it/s]../modules/multihead_attention.py:88: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1614378044816/work/torch/csrc/autograd/variable.cpp:547.)\n",
      "  q *= self.scaling\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.19s/it]\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:2.6149, valid_loss:3.1882, test_acc:0.5000\n",
      "current mae:1.9492, current acc:0.5000, acc7:0.0000, f1:0.5000, corr:0.5293\n",
      "best mae:1.9492, acc:0.5000, acc7:0.0000, f1:0.5000, corr:0.5293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                           | 0/2 [00:00<?, ?it/s]../modules/multihead_attention.py:88: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1614378044816/work/torch/csrc/autograd/variable.cpp:547.)\n",
      "  q *= self.scaling\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.07s/it]\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, train_loss:2.6139, valid_loss:3.1888, test_acc:0.5000\n",
      "current mae:1.9497, current acc:0.5000, acc7:0.0000, f1:0.5000, corr:0.0540\n",
      "best mae:1.9492, acc:0.5000, acc7:0.0000, f1:0.5000, corr:0.5293\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6c617-bc08-4d70-a8a1-7f76fc785aca",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f98294-556d-4df0-8d75-be41ff5194b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dingning/prj4/runs/2mhteaqx?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1394afba8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 149, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 125, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 397, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 222, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 227, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "\n",
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 167, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 114, in communicate_stop_status\n",
      "    resp = self._communicate_stop_status(status)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 387, in _communicate_stop_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 222, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/Users/mac/anaconda3/envs/t18/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\", line 227, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443a07a-2747-4f20-8509-7411c36816b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02f80ddb02fe422b9c91b6279917a5e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "4c1cacd880e240bba1eb0c3bc02312c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7519a6814f664e3d826e89593c1c9b51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_4c1cacd880e240bba1eb0c3bc02312c1",
       "style": "IPY_MODEL_02f80ddb02fe422b9c91b6279917a5e1"
      }
     },
     "75c3c437879f4c9abd5af90851d1e965": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7519a6814f664e3d826e89593c1c9b51",
        "IPY_MODEL_9dc563c944e941bdbee5cdf6c9e4de5b"
       ],
       "layout": "IPY_MODEL_eb363cb2734345b9aabb0fa7c3f6bdb4"
      }
     },
     "780fe38f517042b3a1b5df23bcb5922d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9dc563c944e941bdbee5cdf6c9e4de5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_780fe38f517042b3a1b5df23bcb5922d",
       "max": 1,
       "style": "IPY_MODEL_f4c1c5687ace4a179b95126207e6ef73"
      }
     },
     "eb363cb2734345b9aabb0fa7c3f6bdb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4c1c5687ace4a179b95126207e6ef73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
